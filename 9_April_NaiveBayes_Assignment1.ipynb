{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "062b1c13-a5db-4d67-bd36-d5e904c79d37",
   "metadata": {},
   "source": [
    "### Q1. What is Bayes' theorem?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "833ce297-1e1b-46a6-90d0-3d8b903bfab4",
   "metadata": {},
   "source": [
    "Bayes' theorem is a mathematical formula that describes the relationship between conditional probabilities. It states that the probability of a hypothesis H given some observed evidence E is proportional to the product of the probability of the evidence given the hypothesis (P(E|H)) and the prior probability of the hypothesis (P(H)). Mathematically, Bayes' theorem is expressed as:\n",
    "\n",
    "P(H|E) = P(E|H) * P(H) / P(E)\n",
    "\n",
    "where P(H|E) is the probability of hypothesis H given evidence E, P(E|H) is the probability of evidence E given hypothesis H, P(H) is the prior probability of hypothesis H, and P(E) is the probability of the observed evidence E.\n",
    "\n",
    "Bayes' theorem has important applications in many fields, including statistics, machine learning, artificial intelligence, and decision theory. It is often used in Bayesian inference, which is a statistical method for updating probabilities based on new evidence or data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5918814a-8274-4412-962f-de6194cda595",
   "metadata": {},
   "source": [
    "### Q2. What is the formula for Bayes' theorem?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d07064e-6a11-4a63-be34-1425632841b1",
   "metadata": {},
   "source": [
    "The formula for Bayes' theorem is:\n",
    "\n",
    "P(H|E) = P(E|H) * P(H) / P(E)\n",
    "\n",
    "where:\n",
    "\n",
    "P(H|E) is the probability of hypothesis H given evidence E\n",
    "P(E|H) is the probability of evidence E given hypothesis H\n",
    "P(H) is the prior probability of hypothesis H\n",
    "P(E) is the probability of the observed evidence E\n",
    "In words, Bayes' theorem states that the probability of a hypothesis given some observed evidence is proportional to the likelihood of the evidence given the hypothesis, multiplied by the prior probability of the hypothesis, and divided by the probability of the observed evidence."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffa48336-b7d5-49cb-9f62-897879b44f8a",
   "metadata": {},
   "source": [
    "### Q3. How is Bayes' theorem used in practice?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d78ca65d-32e5-441d-be18-1e17c51e579b",
   "metadata": {},
   "source": [
    "Bayes' theorem is used in practice in a wide range of fields, including statistics, machine learning, artificial intelligence, decision theory, and more. Some common applications of Bayes' theorem include:\n",
    "\n",
    "Bayesian inference: Bayes' theorem is the foundation of Bayesian inference, which is a statistical method for updating probabilities based on new evidence or data. In Bayesian inference, Bayes' theorem is used to calculate the posterior probability of a hypothesis given some observed data.\n",
    "\n",
    "Medical diagnosis: Bayes' theorem is used in medical diagnosis to calculate the probability of a disease given some observed symptoms. For example, a doctor may use Bayes' theorem to calculate the probability of a patient having cancer given their age, gender, family history, and other risk factors.\n",
    "\n",
    "Spam filtering: Bayes' theorem is used in spam filtering to classify emails as either spam or non-spam. In this application, Bayes' theorem is used to calculate the probability that an email is spam given the words and phrases it contains.\n",
    "\n",
    "Image recognition: Bayes' theorem is used in image recognition to classify images into different categories. In this application, Bayes' theorem is used to calculate the probability that an image belongs to a particular category given its features and characteristics.\n",
    "\n",
    "Prediction and decision-making: Bayes' theorem can be used to make predictions and decisions based on uncertain information. For example, a stock trader may use Bayes' theorem to calculate the probability of a stock increasing in value given some market data and news.\n",
    "\n",
    "Overall, Bayes' theorem provides a powerful framework for reasoning under uncertainty and is widely used in many different fields and applications."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29e01698-f250-4eef-a0b0-25bc3abf6b1d",
   "metadata": {},
   "source": [
    "### Q4. What is the relationship between Bayes' theorem and conditional probability?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8960d182-91c4-4c17-b577-6d8ad47951c0",
   "metadata": {},
   "source": [
    "Bayes' theorem is a formula that describes the relationship between conditional probabilities. In particular, it provides a way to calculate the probability of a hypothesis given some observed evidence, by using conditional probabilities.\n",
    "\n",
    "Conditional probability is the probability of an event (such as observing some evidence) given that another event (such as a hypothesis) has occurred. It is denoted as P(A|B), which is the probability of A given B. Bayes' theorem relates conditional probabilities of two events in opposite directions. It can be derived from the definition of conditional probability as follows:\n",
    "\n",
    "P(H|E) = P(E|H) * P(H) / P(E)\n",
    "\n",
    "where:\n",
    "\n",
    "P(H|E) is the probability of hypothesis H given evidence E\n",
    "P(E|H) is the probability of evidence E given hypothesis H\n",
    "P(H) is the prior probability of hypothesis H\n",
    "P(E) is the probability of the observed evidence E\n",
    "The formula shows that the probability of the hypothesis H given evidence E is proportional to the product of the probability of the evidence given the hypothesis (P(E|H)) and the prior probability of the hypothesis (P(H)), and is divided by the probability of the observed evidence (P(E)).\n",
    "\n",
    "In summary, Bayes' theorem uses conditional probabilities to calculate the probability of a hypothesis given observed evidence, and it provides a powerful tool for reasoning under uncertainty in many fields."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbad02e9-0884-4196-8b05-d619ce390f70",
   "metadata": {},
   "source": [
    "### Q5. How do you choose which type of Naive Bayes classifier to use for any given problem?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bd9fde9-d288-41de-8d4a-a47bd522be19",
   "metadata": {},
   "source": [
    "Choosing the type of Naive Bayes classifier to use for a given problem depends on the nature of the data and the specific requirements of the problem. There are three main types of Naive Bayes classifiers: Gaussian Naive Bayes, Multinomial Naive Bayes, and Bernoulli Naive Bayes. Here are some guidelines on when to use each type:\n",
    "\n",
    "Gaussian Naive Bayes: This classifier is used for continuous numerical data that follow a normal distribution. It assumes that the data is normally distributed within each class, with a mean and variance. Gaussian Naive Bayes is often used in classification problems involving real-valued features, such as image recognition or sentiment analysis.\n",
    "\n",
    "Multinomial Naive Bayes: This classifier is used for discrete count data, such as word counts in text classification problems. It is typically used in problems where each feature represents the frequency of a word or term in a document, and the goal is to classify the document into one of several categories.\n",
    "\n",
    "Bernoulli Naive Bayes: This classifier is similar to Multinomial Naive Bayes, but it is used for binary or boolean data. It is often used in text classification problems where the presence or absence of a word or term is used as a feature.\n",
    "\n",
    "In general, the choice of Naive Bayes classifier depends on the nature of the data and the assumptions that are reasonable for the problem at hand. It is also important to consider the size of the dataset and the computational resources available, as some classifiers may be more computationally intensive than others. Finally, it is often a good practice to compare the performance of different classifiers on a validation dataset before selecting the best one for the task."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fb0bffe-35c6-45c8-9779-f983c551da94",
   "metadata": {},
   "source": [
    "### Q6. Assignment:\n",
    "You have a dataset with two features, X1 and X2, and two possible classes, A and B. You want to use Naive\n",
    "Bayes to classify a new instance with features X1 = 3 and X2 = 4. The following table shows the frequency of\n",
    "each feature value for each class:\n",
    "\n",
    "Class X1=1 X1=2 X1=3 X2=1 X2=2 X2=3 X2=4\n",
    "\n",
    "A 3 3 4 4 3 3 3\n",
    "\n",
    "B 2 2 1 2 2 2 3\n",
    "\n",
    "Assuming equal prior probabilities for each class, which class would Naive Bayes predict the new instance\n",
    "to belong to?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ebd2d7f-12b3-408e-bcc2-9dee61b861d0",
   "metadata": {},
   "source": [
    "To classify the new instance with features X1=3 and X2=4 using Naive Bayes, we need to calculate the posterior probability of each class given the features, and then choose the class with the highest probability. We can use the Naive Bayes formula:\n",
    "\n",
    "P(class | X1=3, X2=4) = P(X1=3, X2=4 | class) * P(class) / P(X1=3, X2=4)\n",
    "\n",
    "where P(X1=3, X2=4 | class) is the likelihood of the features given the class, P(class) is the prior probability of the class, and P(X1=3, X2=4) is the marginal probability of the features.\n",
    "\n",
    "Since the features are assumed to be independent given the class (which is the Naive Bayes assumption), we can calculate the likelihood as:\n",
    "\n",
    "P(X1=3, X2=4 | class) = P(X1=3 | class) * P(X2=4 | class)\n",
    "\n",
    "Using the table, we can calculate the probabilities as follows:\n",
    "\n",
    "P(X1=3 | A) = 4/10\n",
    "P(X1=3 | B) = 1/7\n",
    "P(X2=4 | A) = 3/10\n",
    "P(X2=4 | B) = 1/7\n",
    "P(A) = 1/2\n",
    "P(B) = 1/2\n",
    "\n",
    "The marginal probability of the features can be calculated as:\n",
    "\n",
    "P(X1=3, X2=4) = P(X1=3, X2=4 | A) * P(A) + P(X1=3, X2=4 | B) * P(B)\n",
    "= (4/10 * 3/10 * 1/2) + (1/7 * 1/7 * 1/2)\n",
    "= 0.01571\n",
    "\n",
    "Using Bayes' theorem, we can calculate the posterior probabilities as follows:\n",
    "\n",
    "P(A | X1=3, X2=4) = (4/10 * 3/10 * 1/2) / 0.01571\n",
    "= 0.7586\n",
    "\n",
    "P(B | X1=3, X2=4) = (1/7 * 1/7 * 1/2) / 0.01571\n",
    "= 0.2414\n",
    "\n",
    "Therefore, Naive Bayes would predict that the new instance belongs to class A, since it has a higher posterior probability (0.7586) than class B (0.2414)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dea7532-c362-4739-939e-851c89bbad38",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
