{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8ccae226-824b-4cfa-a4b7-bd2c960b9391",
   "metadata": {},
   "source": [
    "### Q1. A company conducted a survey of its employees and found that 70% of the employees use the\n",
    "company's health insurance plan, while 40% of the employees who use the plan are smokers. What is the\n",
    "probability that an employee is a smoker given that he/she uses the health insurance plan?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d9d5574-bf20-432b-b134-77fc90aad000",
   "metadata": {},
   "source": [
    "To find the probability that an employee is a smoker given that he/she uses the health insurance plan, we need to use Bayes' theorem, which states:\n",
    "\n",
    "P(A|B) = P(B|A) * P(A) / P(B)\n",
    "\n",
    "Where:\n",
    "\n",
    "P(A|B) is the probability of event A occurring given that event B has occurred\n",
    "P(B|A) is the probability of event B occurring given that event A has occurred\n",
    "P(A) is the probability of event A occurring\n",
    "P(B) is the probability of event B occurring\n",
    "In this case, event A is \"being a smoker\" and event B is \"using the health insurance plan\".\n",
    "\n",
    "We are given that:\n",
    "\n",
    "P(B) = 0.7 (70% of employees use the health insurance plan)\n",
    "P(A|B) = ?\n",
    "P(B|A) = 0.4 (40% of employees who use the plan are smokers)\n",
    "P(A) = ? (we don't have this information)\n",
    "To find P(A), we need to use the law of total probability:\n",
    "\n",
    "P(A) = P(A|B) * P(B) + P(A|not B) * P(not B)\n",
    "\n",
    "Where:\n",
    "\n",
    "P(A|not B) is the probability of event A occurring given that event B has not occurred\n",
    "P(not B) is the probability of event B not occurring, which is equal to 1 - P(B)\n",
    "We are not given P(A|not B), but we can assume that it is lower than P(A|B), since the use of health insurance plan may be positively associated with smoking. Therefore, we can use a conservative estimate and assume P(A|not B) = 0.2.\n",
    "\n",
    "Using the above equation, we can calculate P(A):\n",
    "\n",
    "P(A) = P(A|B) * P(B) + P(A|not B) * P(not B)\n",
    "= P(A|B) * 0.7 + 0.2 * 0.3\n",
    "= 0.49 + 0.06\n",
    "= 0.55\n",
    "\n",
    "Now we can use Bayes' theorem to find P(A|B):\n",
    "\n",
    "P(A|B) = P(B|A) * P(A) / P(B)\n",
    "= 0.4 * 0.55 / 0.7\n",
    "= 0.314\n",
    "\n",
    "Therefore, the probability that an employee is a smoker given that he/she uses the health insurance plan is 0.314 or approximately 31.4%."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f10a7585-1eaa-45e8-9389-256efb662ad9",
   "metadata": {},
   "source": [
    "### Q2. What is the difference between Bernoulli Naive Bayes and Multinomial Naive Bayes?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98799300-f15d-4714-b479-8bb72b2b8a53",
   "metadata": {},
   "source": [
    "Bernoulli Naive Bayes and Multinomial Naive Bayes are two variants of Naive Bayes algorithm that are commonly used in text classification and natural language processing.\n",
    "\n",
    "The main difference between these two algorithms is the type of input data they are designed to work with. Bernoulli Naive Bayes is used for binary or Boolean data, while Multinomial Naive Bayes is used for count-based data.\n",
    "\n",
    "Bernoulli Naive Bayes assumes that each feature in the input data is binary or Boolean, meaning that it takes on one of two values (e.g., 0 or 1, false or true, etc.). It is commonly used for text classification tasks where the presence or absence of certain words or features is used as input. In this case, each document is represented as a binary vector, where each element corresponds to the presence or absence of a particular word or feature.\n",
    "\n",
    "On the other hand, Multinomial Naive Bayes is used when the input data is represented as count-based features. In text classification, this means that each document is represented as a vector of word counts, where each element corresponds to the number of times a particular word appears in the document. This algorithm assumes that the features are discrete and follow a multinomial distribution, hence the name \"Multinomial\" Naive Bayes.\n",
    "\n",
    "In summary, the key difference between Bernoulli Naive Bayes and Multinomial Naive Bayes is the type of input data they are designed to handle. Bernoulli Naive Bayes works with binary data, while Multinomial Naive Bayes works with count-based data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3a43f6b-7469-42d2-b627-95bf315adcec",
   "metadata": {},
   "source": [
    "### Q3. How does Bernoulli Naive Bayes handle missing values?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e715f172-718b-4d6a-9a67-57429262f347",
   "metadata": {},
   "source": [
    "In Bernoulli Naive Bayes, missing values are typically handled by treating them as a separate category or class. This means that instead of assuming that the missing values are either 0 or 1, we assume that they are a third category, denoted as \"?\", for example.\n",
    "\n",
    "When we train the Bernoulli Naive Bayes model, we estimate the probability of each feature being equal to 0 or 1, as well as the probability of each class (i.e., the target variable). To incorporate the missing values, we also estimate the probability of each feature being equal to \"?\".\n",
    "\n",
    "During prediction, if a missing value is encountered for a particular feature, we can use the probabilities we estimated during training to calculate the probability of the instance belonging to each class, given that the value of the missing feature is unknown. Specifically, we calculate the probability of the instance belonging to each class as follows:\n",
    "\n",
    "P(y | x1, ..., xn, ?) ∝ P(y) * ∏ P(xi | y) for i = 1 to n\n",
    "\n",
    "where:\n",
    "\n",
    "y is the class (target variable)\n",
    "xi is the value of the i-th feature (either 0, 1, or ?)\n",
    "? is the missing value\n",
    "∏ is the product symbol\n",
    "We compute this probability for each class and then select the class with the highest probability as the predicted class.\n",
    "\n",
    "Note that this approach assumes that the missing values are missing completely at random (MCAR). If the missing values are not MCAR, other techniques such as imputation may be more appropriate."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28637bfc-6561-41e3-8ece-98bc1e748901",
   "metadata": {},
   "source": [
    "### Q4. Can Gaussian Naive Bayes be used for multi-class classification?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5152cec0-ce50-46c8-ae80-59cb22a88d28",
   "metadata": {},
   "source": [
    "Yes, Gaussian Naive Bayes can be used for multi-class classification. Gaussian Naive Bayes is a variant of the Naive Bayes algorithm that is used when the features are continuous and follow a Gaussian (i.e., normal) distribution.\n",
    "\n",
    "To use Gaussian Naive Bayes for multi-class classification, we can extend the binary classification model by applying it to each possible pair of classes, and then combining the results. Specifically, for a problem with k classes, we would train k*(k-1)/2 binary classifiers, each one trained to distinguish between two classes.\n",
    "\n",
    "At prediction time, given a new instance, we would apply each binary classifier to it and determine which of the k classes it is most likely to belong to, based on the probabilities output by each binary classifier. We can use a voting scheme, such as selecting the class with the highest probability or using a weighted voting scheme, to combine the outputs of the binary classifiers and obtain the final predicted class.\n",
    "\n",
    "Alternatively, we can use a one-vs-all (OvA) approach, where we train k binary classifiers, each one trained to distinguish between one class and the rest of the classes combined. At prediction time, we apply each binary classifier to the new instance and select the class that has the highest probability output by any of the binary classifiers.\n",
    "\n",
    "Overall, while Gaussian Naive Bayes was originally designed for binary classification problems, it can be extended to handle multi-class classification problems."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4e904df-241f-44c0-9ca1-8d943fc9d123",
   "metadata": {},
   "source": [
    "### Q5. Assignment:\n",
    "    \n",
    "Data preparation:\n",
    "    \n",
    "Download the \"Spambase Data Set\" from the UCI Machine Learning Repository (https://archive.ics.uci.edu/ml/\n",
    "datasets/Spambase). This dataset contains email messages, where the goal is to predict whether a message\n",
    "is spam or not based on several input features.\n",
    "\n",
    "Implementation:\n",
    "    \n",
    "Implement Bernoulli Naive Bayes, Multinomial Naive Bayes, and Gaussian Naive Bayes classifiers using the\n",
    "scikit-learn library in Python. Use 10-fold cross-validation to evaluate the performance of each classifier on the\n",
    "dataset. You should use the default hyperparameters for each classifier.\n",
    "\n",
    "Results:\n",
    "    \n",
    "Report the following performance metrics for each classifier:\n",
    "Accuracy\n",
    "Precision\n",
    "Recall\n",
    "F1 score\n",
    "\n",
    "Discussion:\n",
    "    \n",
    "Discuss the results you obtained. Which variant of Naive Bayes performed the best? Why do you think that is\n",
    "the case? Are there any limitations of Naive Bayes that you observed?\n",
    "\n",
    "Conclusion:\n",
    "    \n",
    "Summarise your findings and provide some suggestions for future work.\n",
    "\n",
    "Note: This dataset contains a binary classification problem with multiple features. The dataset is\n",
    "relatively small, but it can be used to demonstrate the performance of the different variants of Naive\n",
    "Bayes on a real-world problem.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6545b9a-a68d-465c-9f41-75c34c73d254",
   "metadata": {},
   "source": [
    "code for implementing Bernoulli Naive Bayes, Multinomial Naive Bayes, and Gaussian Naive Bayes classifiers using scikit-learn in Python, and evaluating their performance on the Spambase dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ca915225-60c6-4774-a925-386c1fd3862f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bernoulli Naive Bayes Classifier:\n",
      "Accuracy: 0.8839380364047911\n",
      "Precision: 0.8860911270983214\n",
      "Recall: 0.815223386651958\n",
      "F1 Score: 0.8491812697500718\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import BernoulliNB\n",
    "from sklearn.model_selection import cross_val_score\n",
    "import pandas as pd\n",
    "\n",
    "# Load the dataset\n",
    "data = pd.read_csv('spambase.data', header=None)\n",
    "\n",
    "# Split the dataset into features and labels\n",
    "X = data.iloc[:,:-1]\n",
    "y = data.iloc[:,-1]\n",
    "\n",
    "# Initialize the Bernoulli Naive Bayes classifier\n",
    "bnb = BernoulliNB()\n",
    "\n",
    "# Fit the classifier on the training data\n",
    "bnb.fit(X, y)\n",
    "\n",
    "# Evaluate the performance of the classifier using 10-fold cross-validation\n",
    "scores = cross_val_score(bnb, X, y, cv=10)\n",
    "\n",
    "# Report the performance metrics\n",
    "print(\"Bernoulli Naive Bayes Classifier:\")\n",
    "print(\"Accuracy:\", scores.mean())\n",
    "print(\"Precision:\", precision_score(y, bnb.predict(X)))\n",
    "print(\"Recall:\", recall_score(y, bnb.predict(X)))\n",
    "print(\"F1 Score:\", f1_score(y, bnb.predict(X)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee4a28d8-2acf-422c-be6f-c57c388968d5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
