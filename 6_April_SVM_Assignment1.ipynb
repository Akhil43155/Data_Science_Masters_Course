{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "88b00e30-b617-4fea-ade2-1001804e7141",
   "metadata": {},
   "source": [
    "### Q1. What is the mathematical formula for a linear SVM?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8ac675c-d462-48e5-bb97-b44a694fb6bd",
   "metadata": {},
   "source": [
    "A linear SVM (Support Vector Machine) is a type of binary classifier that separates two classes by finding the hyperplane with the maximum margin between them. The mathematical formula for a linear SVM can be expressed as follows:\n",
    "\n",
    "Given a training dataset of labeled points (x1, y1), (x2, y2), ..., (xn, yn), where xi is the input feature vector of dimension d, and yi is either +1 or -1 denoting the class label of the corresponding point, the objective of a linear SVM is to find the hyperplane w.x + b = 0 that maximizes the margin between the two classes.\n",
    "\n",
    "Here, w is the weight vector perpendicular to the hyperplane, and b is the bias term.\n",
    "\n",
    "The optimization problem for a linear SVM can be formulated as follows:\n",
    "\n",
    "minimize ||w|| subject to yi(w.xi + b) >= 1 for all i\n",
    "\n",
    "where ||w|| is the L2-norm of the weight vector w, and yi(w.xi + b) is the margin of the i-th point with respect to the hyperplane.\n",
    "\n",
    "The solution to the optimization problem is the weight vector w and the bias term b that satisfy the above constraints and maximize the margin. Once the optimal hyperplane is found, the classification of a new point x is performed by evaluating the sign of w.x + b. If the result is positive, the point is assigned to one class, and if it is negative, the point is assigned to the other class."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a865c558-50d9-4bfd-b8a7-30ba64feae3f",
   "metadata": {},
   "source": [
    "### Q2. What is the objective function of a linear SVM?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "992ced68-4b37-4a1a-bf5b-a0c37b97e221",
   "metadata": {},
   "source": [
    "The objective function of a linear SVM (Support Vector Machine) is to find the hyperplane that maximizes the margin between two classes. The margin is the distance between the hyperplane and the closest points of each class, also known as support vectors.\n",
    "\n",
    "Given a training dataset of labeled points (x1, y1), (x2, y2), ..., (xn, yn), where xi is the input feature vector of dimension d, and yi is either +1 or -1 denoting the class label of the corresponding point, the objective of a linear SVM can be formulated as follows:\n",
    "\n",
    "minimize (1/2)||w||^2 subject to yi(w.xi + b) >= 1 for all i\n",
    "\n",
    "where ||w|| is the L2-norm of the weight vector w, and yi(w.xi + b) is the margin of the i-th point with respect to the hyperplane. The constant 1/2 is used for mathematical convenience and does not affect the solution.\n",
    "\n",
    "The objective function can be interpreted as finding the hyperplane that separates the two classes with the largest margin while ensuring that all the data points are correctly classified. The constraint yi(w.xi + b) >= 1 ensures that the margin is at least 1, and therefore, the hyperplane is not too sensitive to small perturbations in the data.\n",
    "\n",
    "The solution to the optimization problem is the weight vector w and the bias term b that satisfy the above constraints and maximize the margin. Once the optimal hyperplane is found, the classification of a new point x is performed by evaluating the sign of w.x + b. If the result is positive, the point is assigned to one class, and if it is negative, the point is assigned to the other class."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b226ba44-ba62-4540-ab8a-9963ae182c15",
   "metadata": {},
   "source": [
    "### Q3. What is the kernel trick in SVM?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0dfb51d-c9f7-4dcf-9dd7-d67b5f9b656d",
   "metadata": {},
   "source": [
    "The kernel trick is a technique used in SVM (Support Vector Machine) to extend the linear classifier to a nonlinear one, without explicitly computing the nonlinear feature space. It is used to find a decision boundary that can separate the data in a high-dimensional space.\n",
    "\n",
    "The idea behind the kernel trick is to transform the input data into a higher-dimensional space where a linear boundary can be used to separate the data. This is done by replacing the dot product between two data points with a kernel function that computes the similarity between the data points in the high-dimensional space. The kernel function is a mathematical function that takes two vectors as input and returns a scalar value that represents their similarity.\n",
    "\n",
    "The most commonly used kernel functions are the linear kernel, the polynomial kernel, and the radial basis function (RBF) kernel. The linear kernel corresponds to the dot product between two vectors and is used when the data is linearly separable. The polynomial kernel and the RBF kernel are used when the data is not linearly separable.\n",
    "\n",
    "By using the kernel trick, the SVM can learn a nonlinear decision boundary in the high-dimensional feature space without explicitly computing the coordinates of the data points in the feature space. This is computationally efficient and avoids the curse of dimensionality problem that arises when the dimensionality of the feature space is large.\n",
    "\n",
    "The kernel trick is a powerful technique that allows SVM to be applied to a wide range of applications, including image recognition, text classification, and bioinformatics."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a5ccd36-6cfd-4692-9f74-bce8617c3eef",
   "metadata": {},
   "source": [
    "### Q4. What is the role of support vectors in SVM Explain with example"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84c4dd98-eed3-45d2-9d83-8533108b9474",
   "metadata": {},
   "source": [
    "Support vectors play a crucial role in SVM (Support Vector Machine) as they are the data points closest to the decision boundary or the hyperplane. These points are used to define the decision boundary and determine the margin of the SVM.\n",
    "\n",
    "In SVM, the goal is to find the hyperplane that separates the two classes of data with the maximum margin, where the margin is the distance between the hyperplane and the closest data points of each class. The data points that lie on the margin or the closest to the margin are known as support vectors.\n",
    "\n",
    "Support vectors are important because they determine the position of the decision boundary, and any change in their position could result in a different decision boundary. Therefore, the support vectors define the SVM and have a significant impact on its performance.\n",
    "\n",
    "For example, consider a binary classification problem where the goal is to classify images of cats and dogs. The SVM algorithm tries to find a hyperplane that separates the two classes of images with the maximum margin. The images that are closest to the hyperplane, or lie on the margin, are the support vectors. These images are the most informative and are used to define the decision boundary.\n",
    "\n",
    "If the position of the support vectors changes, the decision boundary of the SVM also changes. Therefore, the choice of support vectors is critical to the performance of the SVM. In practice, the SVM algorithm selects the support vectors automatically during the training process, based on their distance from the decision boundary."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4382b4e4-5aa1-44d4-b60d-8469573501b1",
   "metadata": {},
   "source": [
    "### Q5. Illustrate with examples and graphs of Hyperplane, Marginal plane, Soft margin and Hard margin in\n",
    "SVM?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efdcb1ca-1aca-44e1-841f-91b8b48cca74",
   "metadata": {},
   "source": [
    "Hyperplane:\n",
    "In SVM, the hyperplane is the decision boundary that separates the data points of different classes. In a binary classification problem, the hyperplane is a linear boundary that separates the data points into two classes. For example, consider the following dataset of two classes, where the blue circles represent class 0, and the red triangles represent class 1.\n",
    "svm_hyperplane_1\n",
    "\n",
    "The hyperplane that separates the two classes is a line in this case, represented by the equation w.x + b = 0, where w is the weight vector, x is the input feature vector, and b is the bias term.\n",
    "\n",
    "Marginal plane:\n",
    "The marginal plane in SVM is the plane that is parallel to the hyperplane and passes through the support vectors. It is used to define the margin, which is the distance between the hyperplane and the closest data points of each class. For example, consider the following dataset, where the two classes are not linearly separable.\n",
    "svm_marginal_plane_1\n",
    "\n",
    "In this case, we can use the kernel trick to transform the data into a higher-dimensional space where it is separable. The hyperplane in this space is a plane, and the marginal plane is the parallel plane that passes through the support vectors.\n",
    "\n",
    "svm_marginal_plane_2\n",
    "\n",
    "Hard margin:\n",
    "A hard margin SVM is an SVM with no tolerance for misclassification. It tries to find the hyperplane that separates the data with the maximum margin while ensuring that all the data points are correctly classified. For example, consider the following dataset, where the two classes are linearly separable.\n",
    "svm_hard_margin_1\n",
    "\n",
    "In this case, the hard margin SVM tries to find the hyperplane that separates the two classes with the maximum margin while ensuring that all the data points are correctly classified.\n",
    "\n",
    "svm_hard_margin_2\n",
    "\n",
    "Soft margin:\n",
    "A soft margin SVM is an SVM with some tolerance for misclassification. It allows some data points to be misclassified to find a more generalizable decision boundary. For example, consider the following dataset, where the two classes are not linearly separable.\n",
    "svm_soft_margin_1\n",
    "\n",
    "In this case, a hard margin SVM cannot be used since the data is not linearly separable. Instead, a soft margin SVM can be used to find the hyperplane that separates the two classes with a margin that allows some misclassification. The trade-off between the margin size and the number of misclassified points is controlled by a parameter C, which determines the penalty for misclassification.\n",
    "\n",
    "svm_soft_margin_2\n",
    "\n",
    "In summary, the concepts of Hyperplane, Marginal plane, Soft margin, and Hard margin are important in understanding how SVM works and how to choose the appropriate parameters for a given dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6df7918-b181-48e6-8294-a8a48a608de1",
   "metadata": {},
   "source": [
    "### Q6. SVM Implementation through Iris dataset.\n",
    "    ~ Load the iris dataset from the scikit-learn library and split it into a training set and a testing setl\n",
    "\t~ Train a linear SVM classifier on the training set and predict the labels for the testing setl\n",
    "\t~ Compute the accuracy of the model on the testing setl\n",
    "\t~ Plot the decision boundaries of the trained model using two of the featuresl\n",
    "\t~ Try different values of the regularisation parameter C and see how it affects the performance of\n",
    "\tthe model.\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "11eef964-c55b-4545-893a-203b9da9562a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 1.0\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Load iris dataset\n",
    "iris = load_iris()\n",
    "X = iris.data\n",
    "y = iris.target\n",
    "\n",
    "# Split data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Train linear SVM classifier\n",
    "svm = SVC(kernel='linear')\n",
    "svm.fit(X_train, y_train)\n",
    "\n",
    "# Predict labels for testing set\n",
    "y_pred = svm.predict(X_test)\n",
    "\n",
    "# Compute accuracy of model on testing set\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print('Accuracy:', accuracy)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfbc7d36-2407-4d71-8721-98f76d08a3cb",
   "metadata": {},
   "source": [
    "### Bonus task: Implement a linear SVM classifier from scratch using Python and compare its\n",
    "performance with the scikit-learn implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "360993c4-52b6-40b4-aec6-e5026b351497",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class LinearSVM:\n",
    "    def __init__(self, lr=0.01, num_iters=1000, C=1.0):\n",
    "        self.lr = lr\n",
    "        self.num_iters = num_iters\n",
    "        self.C = C\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        n_samples, n_features = X.shape\n",
    "        \n",
    "        # Initialize parameters\n",
    "        self.w = np.zeros(n_features)\n",
    "        self.b = 0\n",
    "        \n",
    "        # Gradient descent\n",
    "        for _ in range(self.num_iters):\n",
    "            for i, x_i in enumerate(X):\n",
    "                cond = y[i] * (np.dot(x_i, self.w) - self.b) >= 1\n",
    "                \n",
    "                if cond:\n",
    "                    self.w -= self.lr * (2 * self.C * self.w)\n",
    "                else:\n",
    "                    self.w -= self.lr * (2 * self.C * self.w - np.dot(x_i, y[i]))\n",
    "                    self.b -= self.lr * y[i]\n",
    "    \n",
    "    def predict(self, X):\n",
    "        linear_output = np.dot(X, self.w) - self.b\n",
    "        return np.sign(linear_output)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29b7f209-80d7-4957-9183-be48ed51a6e2",
   "metadata": {},
   "source": [
    "This code implements a linear SVM classifier using gradient descent to find the optimal values for the parameters w and b. The fit method updates the parameters using the gradient descent algorithm, while the predict method computes the linear output of the SVM and returns the predicted class labels.\n",
    "\n",
    "To compare the performance of this implementation with scikit-learn, we can use the same iris dataset and split it into training and testing sets, train both the custom implementation and scikit-learn implementation of linear SVM classifier on the training set, and evaluate their performance on the testing set. Here's an example code to do that:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fbf51744-d121-45f0-9117-123f5908ef44",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Custom Linear SVM Accuracy: 0.3\n",
      "Scikit-learn Linear SVM Accuracy: 1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Load iris dataset\n",
    "iris = load_iris()\n",
    "X = iris.data\n",
    "y = iris.target\n",
    "\n",
    "# Split data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Train custom linear SVM classifier\n",
    "svm_custom = LinearSVM(lr=0.01, num_iters=1000, C=1.0)\n",
    "svm_custom.fit(X_train, y_train)\n",
    "y_pred_custom = svm_custom.predict(X_test)\n",
    "\n",
    "# Train scikit-learn linear SVM classifier\n",
    "svm_sklearn = LinearSVC(C=1.0)\n",
    "svm_sklearn.fit(X_train, y_train)\n",
    "y_pred_sklearn = svm_sklearn.predict(X_test)\n",
    "\n",
    "# Compute accuracy of models on testing set\n",
    "accuracy_custom = accuracy_score(y_test, y_pred_custom)\n",
    "accuracy_sklearn = accuracy_score(y_test, y_pred_sklearn)\n",
    "\n",
    "print('Custom Linear SVM Accuracy:', accuracy_custom)\n",
    "print('Scikit-learn Linear SVM Accuracy:', accuracy_sklearn)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa5d400b-1880-4429-81bf-75f68c5acf91",
   "metadata": {},
   "source": [
    "This code trains both the custom implementation and scikit-learn implementation of linear SVM classifier on the same iris dataset, and computes their accuracy on the testing set. By comparing the accuracy scores of both models, we can see how well the custom implementation performs compared to the scikit-learn implementation.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ad0b7be-9d1b-42c7-a3af-81466df5e3b0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6082554e-472f-4400-aaff-40cafdb2980e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
