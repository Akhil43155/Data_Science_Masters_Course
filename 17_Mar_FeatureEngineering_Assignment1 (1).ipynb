{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5d54b769-26d2-4112-a60c-68c0a1e99e97",
   "metadata": {},
   "source": [
    "### Q1: What are missing values in a dataset? Why is it essential to handle missing values? Name some\n",
    "algorithms that are not affected by missing values.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e561f299-0482-453f-bafd-936b60291c84",
   "metadata": {},
   "source": [
    "Missing values in a dataset refer to the absence of data points for one or more variables. There can be several reasons for missing values, such as data collection errors, incomplete data, or intentionally not recorded data.\n",
    "\n",
    "Handling missing values is crucial because they can significantly impact the accuracy of data analysis and modeling. Ignoring missing values can lead to biased or incorrect results and can also cause errors in statistical analysis. Therefore, it is essential to handle missing values appropriately to ensure the quality of data analysis.\n",
    "\n",
    "Some algorithms that are not affected by missing values are:\n",
    "\n",
    "Decision Trees: Decision trees can handle missing values by considering all possible splits in the tree and choosing the one that leads to the best classification or prediction.\n",
    "\n",
    "Random Forest: Random forest is an ensemble learning algorithm that builds multiple decision trees and averages their results. It can handle missing values in a similar way as decision trees.\n",
    "\n",
    "K-Nearest Neighbors (KNN): KNN is a non-parametric algorithm that uses the distance between data points to predict the outcome. It can handle missing values by computing the distance between the available data points.\n",
    "\n",
    "Support Vector Machines (SVM): SVM is a classification algorithm that creates a hyperplane to separate the data into different classes. It can handle missing values by considering only the available data points to build the hyperplane."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f81c8b09-ee13-417d-9c4f-7dab724a3f15",
   "metadata": {},
   "source": [
    "### Q2: List down techniques used to handle missing data. Give an example of each with python code.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de59a164-61ea-4373-a2bb-9c981d0d6583",
   "metadata": {},
   "source": [
    "There are several techniques that can be used to handle missing data, and here are some examples along with Python code:\n",
    "\n",
    "Mean/median imputation: In this technique, the missing values are replaced with the mean or median value of the available data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1bf6f3cc-77c4-489b-b121-c7983272dcc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# create a sample dataframe with missing values\n",
    "df = pd.DataFrame({'A': [1, 2, np.nan, 4], 'B': [5, np.nan, 7, 8]})\n",
    "\n",
    "# fill the missing values with the mean of each column\n",
    "df.fillna(df.mean(), inplace=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64bc01f8-e54c-41e3-8094-aa68635ddc84",
   "metadata": {},
   "source": [
    "Forward/Backward fill imputation: In this technique, the missing values are replaced with the previous or next available value in the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "394212d1-10ed-4e14-8a7d-f2e3a2f7ddcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# create a sample dataframe with missing values\n",
    "df = pd.DataFrame({'A': [1, np.nan, 3, np.nan, 5], 'B': [6, 7, np.nan, 9, np.nan]})\n",
    "\n",
    "# fill the missing values using forward fill\n",
    "df.fillna(method='ffill', inplace=True)\n",
    "\n",
    "# fill the missing values using backward fill\n",
    "df.fillna(method='bfill', inplace=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45ab761a-2004-4d74-86af-feb8bcf04d0c",
   "metadata": {},
   "source": [
    "K-nearest neighbors imputation: In this technique, the missing values are replaced with the values of the nearest neighbors in the available data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4eb3cbce-46b0-4f13-b85c-b588ebd3d7cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.impute import KNNImputer\n",
    "\n",
    "# create a sample dataframe with missing values\n",
    "df = pd.DataFrame({'A': [1, 2, np.nan, 4], 'B': [5, np.nan, 7, 8]})\n",
    "\n",
    "# impute the missing values using K-nearest neighbors\n",
    "imputer = KNNImputer(n_neighbors=2)\n",
    "df_imputed = imputer.fit_transform(df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0e89654-79e7-4b06-956c-794a04bd955f",
   "metadata": {},
   "source": [
    "Multiple imputation: In this technique, multiple imputed datasets are generated, and the analysis is performed on each of them, and the results are combined."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e1e07749-e37d-4168-b9b8-e93a3c1618df",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.experimental import enable_iterative_imputer\n",
    "from sklearn.impute import IterativeImputer\n",
    "\n",
    "# create a sample dataframe with missing values\n",
    "df = pd.DataFrame({'A': [1, 2, np.nan, 4], 'B': [5, np.nan, 7, 8]})\n",
    "\n",
    "# impute the missing values using multiple imputation\n",
    "imputer = IterativeImputer(random_state=0)\n",
    "df_imputed = imputer.fit_transform(df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98b8518f-ace9-469d-ae91-bb6c250b243e",
   "metadata": {},
   "source": [
    "### Q3: Explain the imbalanced data. What will happen if imbalanced data is not handled?\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ac22abc-8f0c-421e-be91-3903dbd741fd",
   "metadata": {},
   "source": [
    "Imbalanced data refers to a situation in which the distribution of classes in the target variable is disproportionate. In other words, one class has significantly more or fewer instances than the other class. For example, in a binary classification problem, if the positive class has only 10% of the instances, while the negative class has 90% of the instances, the data is considered imbalanced.\n",
    "\n",
    "If imbalanced data is not handled, it can lead to biased or incorrect results in classification models. The classifier can be more accurate in predicting the majority class, but it can have poor performance on the minority class. In this case, the classifier may misclassify the minority class instances as the majority class, leading to false negatives or false positives. False negatives are particularly dangerous in cases where the minority class represents an important or rare event, such as a disease or a fraud.\n",
    "\n",
    "Moreover, the performance metrics that are commonly used for evaluating classification models, such as accuracy, can be misleading in imbalanced datasets. The classifier can achieve high accuracy by predicting the majority class, but this does not reflect the model's ability to correctly classify the minority class. Therefore, it is crucial to handle imbalanced data to ensure that the classifier performs well on both the majority and minority classes.\n",
    "\n",
    "Some techniques used to handle imbalanced data include:\n",
    "\n",
    "Undersampling the majority class.\n",
    "Oversampling the minority class.\n",
    "Generating synthetic samples using techniques such as SMOTE (Synthetic Minority Over-sampling Technique).\n",
    "Modifying the classification algorithm to incorporate class weights or cost-sensitive learning.\n",
    "Using ensemble methods, such as Random Forest or Gradient Boosting, that can handle imbalanced data by combining multiple models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73486757-1eb9-4cf3-b192-8d1b31dc3b64",
   "metadata": {},
   "source": [
    "### Q4: What are Up-sampling and Down-sampling? Explain with an example when up-sampling and down-\n",
    "sampling are required.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c71b614-f52f-4bbc-a7fc-32acea3aea10",
   "metadata": {},
   "source": [
    "Up-sampling and down-sampling are techniques used to handle imbalanced data in which the distribution of classes is disproportionate.\n",
    "\n",
    "Down-sampling, also known as under-sampling, is a technique that reduces the number of instances in the majority class to balance the class distribution. For example, if we have a dataset with 1000 instances, of which 900 belong to the majority class and 100 belong to the minority class, we can randomly select 100 instances from the majority class to match the number of instances in the minority class. This can help prevent the classifier from being biased towards the majority class.\n",
    "\n",
    "Up-sampling, also known as over-sampling, is a technique that increases the number of instances in the minority class to balance the class distribution. For example, if we have the same dataset as above, we can create synthetic instances of the minority class using techniques such as SMOTE (Synthetic Minority Over-sampling Technique) to match the number of instances in the majority class. This can help improve the classifier's ability to predict the minority class.\n",
    "\n",
    "When to use up-sampling or down-sampling depends on the characteristics of the dataset and the problem at hand.\n",
    "\n",
    "For example, if the minority class represents a rare event that is of high importance, such as a disease or a fraud, we may want to up-sample the minority class to ensure that the classifier can learn from enough positive examples. On the other hand, if the majority class is already large and representative, we may choose to down-sample the majority class to improve the balance of the dataset.\n",
    "\n",
    "Here is an example:\n",
    "\n",
    "Suppose we have a dataset of credit card transactions, where the target variable indicates whether the transaction is fraudulent or not. Suppose further that the fraud cases represent only 1% of the total transactions, and the remaining 99% are legitimate transactions. In this case, the dataset is highly imbalanced, and the classifier may be biased towards predicting that all transactions are legitimate.\n",
    "\n",
    "To handle this imbalanced data, we can up-sample the minority class by generating synthetic examples of fraud cases using SMOTE, or we can down-sample the majority class by randomly selecting a subset of legitimate transactions. The choice between these techniques depends on the specific requirements and constraints of the problem, such as the importance of detecting fraudulent transactions and the size of the original dataset.\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34dc85e4-75d4-40c3-95ec-68d1a0453a8f",
   "metadata": {},
   "source": [
    "### Q5: What is data Augmentation? Explain SMOTE.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f890cecb-08f1-4f24-a981-471412562e73",
   "metadata": {},
   "source": [
    "Data augmentation is a technique used to artificially increase the size of a dataset by creating new, synthetic examples from the existing ones. This can help to improve the performance of machine learning models by providing them with more data to learn from, reducing overfitting, and improving generalization.\n",
    "\n",
    "One popular data augmentation technique is SMOTE (Synthetic Minority Over-sampling Technique), which is used to address the problem of imbalanced data. SMOTE works by generating synthetic examples of the minority class by interpolating between existing minority class samples.\n",
    "\n",
    "Here's how SMOTE works:\n",
    "\n",
    "SMOTE selects a minority class instance x.\n",
    "It identifies its k nearest minority class neighbors.\n",
    "It randomly selects one of the k neighbors, say x'.\n",
    "SMOTE creates a new synthetic instance by interpolating between x and x'. Specifically, it adds a small random fraction of the difference between the feature vectors of x and x' to x. The resulting instance lies on the line segment between x and x' in the feature space.\n",
    "Repeat steps 1-4 until the desired number of synthetic instances is generated.\n",
    "The SMOTE technique creates new, synthetic instances of the minority class, which can help to balance the class distribution and improve the classifier's ability to predict the minority class. By creating new examples from existing ones, SMOTE can help to address the problem of limited data, which is common in many machine learning applications.\n",
    "\n",
    "SMOTE is a popular technique for handling imbalanced datasets and has been shown to improve the performance of many classifiers, including decision trees, random forests, and support vector machines. However, SMOTE may not be suitable for all datasets, and it is essential to evaluate its effectiveness in each specific case."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c161153b-1c39-44ba-a6c7-811d4c9f367e",
   "metadata": {},
   "source": [
    "### Q6: What are outliers in a dataset? Why is it essential to handle outliers?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37ea5e26-1512-4b45-a0e4-4541491229d5",
   "metadata": {},
   "source": [
    "Outliers are data points that deviate significantly from the rest of the dataset. Outliers can be caused by various factors, such as measurement errors, data entry errors, or natural variation in the data. Outliers can have a significant impact on the results of data analysis and machine learning models, which is why it is essential to handle them properly.\n",
    "\n",
    "Handling outliers is important for several reasons:\n",
    "\n",
    "Outliers can distort the results of statistical analysis. Outliers can significantly affect the mean and standard deviation of a dataset, leading to inaccurate conclusions about the data.\n",
    "\n",
    "Outliers can impact the performance of machine learning models. Many machine learning algorithms are sensitive to outliers, and outliers can lead to overfitting or underfitting of the model.\n",
    "\n",
    "Outliers can be indicative of errors in the data. Outliers may be caused by measurement errors or data entry errors, and it is important to identify and correct these errors to ensure the accuracy of the data.\n",
    "\n",
    "There are several techniques for handling outliers, including:\n",
    "\n",
    "Removing outliers: One approach is to simply remove the outliers from the dataset. This can be done by setting a threshold for what constitutes an outlier, such as any data point that falls outside of a certain range.\n",
    "\n",
    "Transforming the data: Another approach is to transform the data to reduce the impact of outliers. For example, we can use a log transformation to reduce the impact of extreme values.\n",
    "\n",
    "Winsorizing: Winsorizing is a technique that involves replacing extreme values with the nearest non-outlying values. For example, we can replace all values above the 95th percentile with the value at the 95th percentile.\n",
    "\n",
    "Robust statistics: Robust statistical methods are designed to be less sensitive to outliers than traditional statistical methods. For example, we can use the median instead of the mean to calculate central tendency.\n",
    "\n",
    "In summary, handling outliers is essential for accurate data analysis and modeling. There are several techniques for handling outliers, and the choice of method depends on the specific characteristics of the data and the problem at hand."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87ece118-2d13-4c3e-be0b-0c2f2fc36aa6",
   "metadata": {},
   "source": [
    "### Q7: You are working on a project that requires analyzing customer data. However, you notice that some of\n",
    "the data is missing. What are some techniques you can use to handle the missing data in your analysis?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a729bbd0-9002-48ab-b1f2-a23d6c2b6355",
   "metadata": {},
   "source": [
    "There are several techniques that can be used to handle missing data in customer data analysis:\n",
    "\n",
    "Deletion: One approach is to simply delete any records that contain missing values. This can be done using list-wise deletion, where entire records with missing values are deleted, or pair-wise deletion, where only the missing values are deleted. However, this approach can result in a loss of data, and the results may not be representative of the population.\n",
    "\n",
    "Imputation: Imputation is a technique that involves replacing missing values with estimated values. There are several methods for imputing missing data, including mean imputation, median imputation, regression imputation, and k-nearest neighbor imputation.\n",
    "\n",
    "Data augmentation: Data augmentation is a technique used to artificially increase the size of a dataset by creating new, synthetic examples from the existing ones. This can be used to generate new values for missing data points.\n",
    "\n",
    "Substitution: Substitution is a technique that involves replacing missing values with a value that is known or estimated from external sources. For example, we can use external data sources, such as census data or demographic data, to estimate missing values.\n",
    "\n",
    "Model-based methods: Model-based methods use statistical models to estimate missing data values. For example, we can use expectation-maximization (EM) algorithms to estimate missing values in a dataset.\n",
    "\n",
    "It is important to choose the appropriate method for handling missing data based on the nature of the data, the size of the dataset, and the objectives of the analysis. A combination of different methods may be used to handle missing data effectively."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adff335c-a705-4c4f-8152-1531cd5e4618",
   "metadata": {},
   "source": [
    "### Q8: You are working with a large dataset and find that a small percentage of the data is missing. What are\n",
    "some strategies you can use to determine if the missing data is missing at random or if there is a pattern\n",
    "to the missing data?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "038a5f56-95d6-4352-9042-7e7dd52b2e47",
   "metadata": {},
   "source": [
    "To determine whether missing data is missing at random (MAR) or missing not at random (MNAR), there are several strategies that can be used:\n",
    "\n",
    "Visual inspection: One way to identify patterns in missing data is to visually inspect the data using graphs or plots. For example, we can create histograms or scatterplots to examine the distribution of missing data.\n",
    "\n",
    "Correlation analysis: Another approach is to examine the correlations between missing values and other variables in the dataset. If there is a strong correlation between the missing values and other variables, then it is likely that the missing data is not missing at random.\n",
    "\n",
    "Statistical tests: Statistical tests can be used to test whether the missing data is missing at random or not. For example, we can use the Little's MCAR test or the pattern-mixture model to test whether the missing data is missing at random.\n",
    "\n",
    "Multiple imputation: Multiple imputation is a technique that can be used to impute missing values in a dataset while accounting for the missing data mechanism. This can be used to estimate the missing values and determine whether the missing data is MAR or MNAR.\n",
    "\n",
    "Domain knowledge: Finally, domain knowledge can be used to determine whether the missing data is likely to be missing at random or not. For example, if the missing data is related to a sensitive topic, such as income or health status, it is likely that the data is not missing at random.\n",
    "\n",
    "By using these strategies, it is possible to identify patterns in missing data and determine whether the missing data is MAR or MNAR. This information can be used to select appropriate methods for handling the missing data and to ensure that the results of the analysis are accurate and unbiased."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66cb4a79-1aa0-493d-a8f9-e1f75c3918ee",
   "metadata": {},
   "source": [
    "### Q9: Suppose you are working on a medical diagnosis project and find that the majority of patients in the\n",
    "dataset do not have the condition of interest, while a small percentage do. What are some strategies you\n",
    "can use to evaluate the performance of your machine learning model on this imbalanced dataset?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11afa068-cb41-4f25-85b2-339b3fd00b33",
   "metadata": {},
   "source": [
    "When working with imbalanced datasets in medical diagnosis projects, some strategies to evaluate the performance of the machine learning model are:\n",
    "\n",
    "Confusion Matrix: A confusion matrix can help to evaluate the performance of the model by showing the true positives, true negatives, false positives, and false negatives.\n",
    "\n",
    "Precision, Recall and F1 Score: The precision score measures the percentage of true positives over the total number of predicted positives. Recall, on the other hand, measures the percentage of true positives over the total number of actual positives. The F1 score is a weighted average of precision and recall. These metrics can help to evaluate the model's performance and to compare different models.\n",
    "\n",
    "ROC Curve and AUC Score: The ROC curve is a graphical representation of the performance of the model, which shows the trade-off between the true positive rate and the false positive rate at different thresholds. The AUC score measures the area under the ROC curve, which indicates the performance of the model.\n",
    "\n",
    "Stratified Sampling: When splitting the dataset into training and testing sets, it is important to use stratified sampling to ensure that the proportion of positive and negative cases is the same in both sets. This can help to avoid bias in the evaluation of the model.\n",
    "\n",
    "Class Weighting: One approach to handle imbalanced datasets is to assign higher weights to the minority class during the training process. This can help the model to focus more on the minority class and improve its performance.\n",
    "\n",
    "Resampling: Another approach to handle imbalanced datasets is to use resampling techniques such as oversampling the minority class or undersampling the majority class. Oversampling techniques can include generating synthetic data using techniques like SMOTE, while undersampling can include dropping examples from the majority class to balance the dataset.\n",
    "\n",
    "It is important to keep in mind that no single strategy may work best for all imbalanced datasets and a combination of multiple strategies may be needed for effective evaluation of the model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62531351-c48d-4440-98f5-7124c115fabf",
   "metadata": {},
   "source": [
    "### Q10: When attempting to estimate customer satisfaction for a project, you discover that the dataset is\n",
    "unbalanced, with the bulk of customers reporting being satisfied. What methods can you employ to\n",
    "balance the dataset and down-sample the majority class?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d1eec57-7272-4a78-9011-202972f75d1b",
   "metadata": {},
   "source": [
    "When dealing with an unbalanced dataset with the majority of customers reporting being satisfied, there are several methods that can be employed to balance the dataset and down-sample the majority class. Some of these methods include:\n",
    "\n",
    "Random Undersampling: This involves randomly selecting a subset of the majority class to create a more balanced dataset. However, this approach may result in a loss of information.\n",
    "\n",
    "Cluster Centroids Undersampling: This approach involves clustering the majority class and selecting representative samples from each cluster to create a balanced dataset. This can help to preserve the information in the majority class while balancing the dataset.\n",
    "\n",
    "Tomek Links Undersampling: This approach involves identifying pairs of samples from the majority and minority classes that are closest to each other and removing the majority class sample to create a more balanced dataset.\n",
    "\n",
    "Random Oversampling: This involves randomly duplicating samples from the minority class to create a more balanced dataset. However, this approach can lead to overfitting and may not be effective if the minority class is too small.\n",
    "\n",
    "Synthetic Minority Over-sampling Technique (SMOTE): This technique involves generating synthetic samples from the minority class by interpolating between existing samples. This can help to increase the size of the minority class and balance the dataset.\n",
    "\n",
    "It is important to note that the choice of method depends on the specific dataset and the problem at hand. It may be necessary to try out different methods and evaluate their effectiveness using appropriate metrics such as accuracy, precision, recall, and F1 score."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed604594-f0af-48eb-ae2a-b31ca2f7adfd",
   "metadata": {},
   "source": [
    "### Q11: You discover that the dataset is unbalanced with a low percentage of occurrences while working on a\n",
    "project that requires you to estimate the occurrence of a rare event. What methods can you employ to\n",
    "balance the dataset and up-sample the minority class?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c929da8-3d9b-40ae-86ba-5c4c424960fa",
   "metadata": {},
   "source": [
    "When dealing with an imbalanced dataset with a low percentage of occurrences for a rare event, there are several methods that can be employed to balance the dataset and up-sample the minority class. Some of these methods include:\n",
    "\n",
    "Random Oversampling: This involves randomly duplicating samples from the minority class to create a more balanced dataset. However, this approach can lead to overfitting and may not be effective if the minority class is too small.\n",
    "\n",
    "Synthetic Minority Over-sampling Technique (SMOTE): This technique involves generating synthetic samples from the minority class by interpolating between existing samples. This can help to increase the size of the minority class and balance the dataset.\n",
    "\n",
    "Adaptive Synthetic Sampling (ADASYN): This technique is an extension of SMOTE that generates more synthetic samples for the minority class that are harder to learn, based on their level of difficulty.\n",
    "\n",
    "Ensemble Methods: Ensemble methods such as Bagging and Boosting can also be used to balance the dataset. Bagging involves randomly sampling subsets of the majority class and combining them with the minority class to create a more balanced dataset. Boosting involves iteratively adjusting the weights of misclassified samples to give more emphasis to the minority class.\n",
    "\n",
    "One-Class Classification: This approach involves training a model on the minority class and treating the majority class as outliers. This can be useful when the majority class is very different from the minority class.\n",
    "\n",
    "It is important to note that the choice of method depends on the specific dataset and the problem at hand. It may be necessary to try out different methods and evaluate their effectiveness using appropriate metrics such as accuracy, precision, recall, and F1 score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c0753a0-4272-4e86-a288-3e8254531804",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
