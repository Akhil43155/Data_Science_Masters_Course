{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "55325272-9042-46c3-b515-8cf50525567f",
   "metadata": {},
   "source": [
    "### Q1. What is Gradient Boosting Regression?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5124483a-a6fc-46c3-9621-a462faf29d6a",
   "metadata": {},
   "source": [
    "Gradient Boosting Regression (GBR) is a powerful machine learning algorithm used for both regression and classification tasks. It is a type of boosting algorithm that involves iteratively adding weak learners to the model to improve its performance.\n",
    "\n",
    "In Gradient Boosting Regression, the weak learners used are typically decision trees. The algorithm works by first fitting a decision tree to the data, then using the residuals (the difference between the predicted values and the true values) from that tree as the target variable for the next decision tree. This process is repeated until a specified number of trees have been added, or until the model's performance stops improving.\n",
    "\n",
    "The \"gradient\" in Gradient Boosting Regression refers to the fact that the algorithm tries to minimize the loss function by iteratively descending the gradient of the function with respect to the model parameters. The gradient descent process involves adjusting the weights of the weak learners in each iteration to reduce the error in the model.\n",
    "\n",
    "GBR is known for its ability to handle complex, non-linear relationships between the predictor variables and the response variable, as well as for its ability to handle missing data and outliers. However, it can be computationally expensive and prone to overfitting if not tuned properly."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a316e57-f8ea-4ebe-a6ec-766a7cc1857b",
   "metadata": {},
   "source": [
    "### Q2. Implement a simple gradient boosting algorithm from scratch using Python and NumPy. Use a\n",
    "simple regression problem as an example and train the model on a small dataset. Evaluate the model's\n",
    "performance using metrics such as mean squared error and R-squared."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d38a12b5-5837-48f2-ba1f-48d06671c736",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class GradientBoostingRegressor:\n",
    "    def __init__(self, n_estimators=100, learning_rate=0.1, max_depth=3):\n",
    "        self.n_estimators = n_estimators\n",
    "        self.learning_rate = learning_rate\n",
    "        self.max_depth = max_depth\n",
    "        self.trees = []\n",
    "        self.training_mean = None\n",
    "        \n",
    "    def fit(self, X, y):\n",
    "        self.training_mean = np.mean(y)\n",
    "        y_pred = np.full_like(y, self.training_mean)\n",
    "        for i in range(self.n_estimators):\n",
    "            residuals = y - y_pred\n",
    "            tree = self.build_tree(X, residuals, self.max_depth)\n",
    "            self.trees.append(tree)\n",
    "            update = self.learning_rate * self.predict(X, tree)\n",
    "            y_pred += update\n",
    "            \n",
    "    def build_tree(self, X, y, max_depth):\n",
    "        if max_depth == 0:\n",
    "            return np.mean(y)\n",
    "        else:\n",
    "            feature_idxs = np.random.choice(X.shape[1], size=1)\n",
    "            best_feature_idx = feature_idxs[0]\n",
    "            best_threshold = None\n",
    "            best_loss = np.inf\n",
    "            for i in feature_idxs:\n",
    "                thresholds = np.unique(X[:, i])\n",
    "                for threshold in thresholds:\n",
    "                    left_idxs = X[:, i] < threshold\n",
    "                    right_idxs = X[:, i] >= threshold\n",
    "                    left_loss = self.compute_loss(y[left_idxs])\n",
    "                    right_loss = self.compute_loss(y[right_idxs])\n",
    "                    loss = left_loss + right_loss\n",
    "                    if loss < best_loss:\n",
    "                        best_feature_idx = i\n",
    "                        best_threshold = threshold\n",
    "                        best_loss = loss\n",
    "            left_idxs = X[:, best_feature_idx] < best_threshold\n",
    "            right_idxs = X[:, best_feature_idx] >= best_threshold\n",
    "            left_subtree = self.build_tree(X[left_idxs, :], y[left_idxs], max_depth-1)\n",
    "            right_subtree = self.build_tree(X[right_idxs, :], y[right_idxs], max_depth-1)\n",
    "            return (best_feature_idx, best_threshold, left_subtree, right_subtree)\n",
    "        \n",
    "    def predict(self, X, tree=None):\n",
    "        if tree is None:\n",
    "            tree = self.trees[-1]\n",
    "        if not isinstance(tree, tuple):\n",
    "            return tree\n",
    "        else:\n",
    "            feature_idx, threshold, left_subtree, right_subtree = tree\n",
    "            if X[feature_idx] < threshold:\n",
    "                return self.predict(X, left_subtree)\n",
    "            else:\n",
    "                return self.predict(X, right_subtree)\n",
    "            \n",
    "    def compute_loss(self, y):\n",
    "        return np.mean(np.square(y - np.mean(y)))\n",
    "    \n",
    "    def predict_all_trees(self, X):\n",
    "        return np.sum([self.learning_rate * self.predict(X, tree) for tree in self.trees], axis=0) + self.training_mean\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab67b2d1-a8f3-4265-ad3c-740eda7abe08",
   "metadata": {},
   "source": [
    "This code defines a class GradientBoostingRegressor that takes in several hyperparameters such as the number of estimators, learning rate, and maximum depth of each decision tree. The fit method trains the model on the input data X and target variable y using gradient boosting. The build_tree method constructs a decision tree by recursively splitting the data based on the feature that results in the lowest loss. The predict method predicts the output value for a given input using a decision tree. The compute_loss method calculates the mean squared error loss for a given set of target values. The predict_all_trees method computes the final prediction for an input by summing the predictions from all decision trees."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "623a3283-fd90-4818-b5b0-ec288191ece8",
   "metadata": {},
   "source": [
    "### Q3. Experiment with different hyperparameters such as learning rate, number of trees, and tree depth to\n",
    "optimise the performance of the model. Use grid search or random search to find the best\n",
    "hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8c9e5643-16f5-4967-a17a-0d23752ab7fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean squared error: 52.67800904196831\n",
      "R^2 score: 0.20601403739042323\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "data_url = \"http://lib.stat.cmu.edu/datasets/boston\"\n",
    "raw_df = pd.read_csv(data_url, sep=\"\\s+\", skiprows=22, header=None)\n",
    "data = np.hstack([raw_df.values[::2, :], raw_df.values[1::2, :2]])\n",
    "target = raw_df.values[1::2, 2]\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "split_idx = int(0.7 * len(data))\n",
    "X_train, y_train = data[:split_idx], target[:split_idx]\n",
    "X_test, y_test = data[split_idx:], target[split_idx:]\n",
    "\n",
    "# Train a gradient boosting regressor on the training data\n",
    "gb = GradientBoostingRegressor(n_estimators=100, learning_rate=0.1, max_depth=3, random_state=42)\n",
    "gb.fit(X_train, y_train)\n",
    "\n",
    "# Predict the housing prices on the testing data\n",
    "y_pred = gb.predict(X_test)\n",
    "\n",
    "# Evaluate the model using mean squared error and R^2 score\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "print(\"Mean squared error:\", mse)\n",
    "print(\"R^2 score:\", r2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65181910-f137-49eb-96a0-8fbace5f24af",
   "metadata": {},
   "source": [
    "In this code, we first load the Boston Housing dataset and split it into training and testing sets. We then define a grid of hyperparameters to search over, including the learning rate, number of estimators (trees), and maximum depth of each tree. We create a GridSearchCV object with the GradientBoostingRegressor as the estimator, and fit it to the training data to search for the best hyperparameters. We then create a new GradientBoostingRegressor object with the best hyperparameters found by the grid search, fit it to the training data, and make predictions on the testing data. Finally, we compute the mean squared error and R-squared score of the predictions and print them to the console.\n",
    "\n",
    "Note that you can also use a random search instead of a grid search by replacing GridSearchCV with RandomizedSearchCV and adjusting the param_distributions parameter accordingly. Random search is often more efficient than grid search for high-dimensional hyperparameter spaces."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b185b833-f60a-48fa-a180-59d1c3646fd4",
   "metadata": {},
   "source": [
    "### Q4. What is a weak learner in Gradient Boosting?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f16e92f-cfce-4dc7-a32e-b897a1fb22e3",
   "metadata": {},
   "source": [
    "In Gradient Boosting, a weak learner is a model that is only slightly better than random guessing. Weak learners are typically simple models, such as decision trees with a low maximum depth or linear regression models with few features. The idea behind using weak learners is that, by combining many of them, a strong predictor can be created. In each iteration of the boosting algorithm, a new weak learner is trained to correct the errors made by the previous learners. The output of each weak learner is then combined with the output of the previous learners to create a final prediction. The strength of the final model is determined by the number of weak learners used and their individual performance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81be3e3f-88f4-4241-85c2-1f72895ad11c",
   "metadata": {},
   "source": [
    "### Q5. What is the intuition behind the Gradient Boosting algorithm?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9e02a1c-eea2-45ac-a40d-41a4ae90e62c",
   "metadata": {},
   "source": [
    "The intuition behind Gradient Boosting is to iteratively improve a weak model by adding a new model that corrects the errors of the previous model. In other words, it combines multiple weak models to create a strong model.\n",
    "\n",
    "The algorithm works by first fitting a weak model to the data, typically a decision tree with only a few levels. It then calculates the difference between the actual target values and the predicted values from the weak model, and this difference becomes the target variable for the next weak model. The new weak model is then fitted to the residuals, and the process is repeated until the desired level of performance is achieved.\n",
    "\n",
    "The key idea behind Gradient Boosting is that each weak model should be optimized to correct the errors of the previous model. This is achieved by using a gradient descent optimization algorithm to update the parameters of each weak model. The gradient descent algorithm minimizes a cost function, which is typically the mean squared error between the predicted and actual values. By iteratively optimizing the weak models, Gradient Boosting can create a strong model that is highly accurate at making predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4bb4be5-f73d-4ded-b6e2-cf6db3f49349",
   "metadata": {},
   "source": [
    "### Q6. How does Gradient Boosting algorithm build an ensemble of weak learners?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9875013-3f87-4374-b621-5297fa1fa724",
   "metadata": {},
   "source": [
    "The Gradient Boosting algorithm builds an ensemble of weak learners in a stage-wise manner. The process can be summarized as follows:\n",
    "\n",
    "The first weak learner (usually a decision tree) is trained on the original data set.\n",
    "\n",
    "The predictions of the first weak learner are used to identify the instances that the model has not predicted correctly. These instances are given a higher weight in the subsequent iteration.\n",
    "\n",
    "In the subsequent iteration, a new weak learner is trained on the same data set with the adjusted weights.\n",
    "\n",
    "The predictions of this new weak learner are combined with the predictions of the previous weak learner using a weighted sum. The weights are determined by the learning rate (a hyperparameter of the algorithm) and the performance of the new weak learner on the data set.\n",
    "\n",
    "The process continues until a stopping criterion is met (e.g., a maximum number of iterations or the accuracy of the model reaches a certain threshold).\n",
    "\n",
    "The final ensemble of weak learners is obtained by combining the predictions of all the weak learners using a weighted sum, where the weights are determined by the learning rate and the performance of the corresponding weak learner on the data set.\n",
    "\n",
    "The intuition behind this process is that the subsequent weak learners are trained on the instances that the previous learners have not predicted correctly, so they can learn to correct the errors made by the previous learners. The final ensemble of weak learners is able to capture the complex non-linear relationships in the data set and make accurate predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42dff076-a73a-419f-8c88-85327d851fe2",
   "metadata": {},
   "source": [
    "### Q7. What are the steps involved in constructing the mathematical intuition of Gradient Boosting\n",
    "algorithm?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a32b05a0-a89c-4f1c-ba8b-2e988d9c46a0",
   "metadata": {},
   "source": [
    "The mathematical intuition behind Gradient Boosting algorithm can be broken down into the following steps:\n",
    "\n",
    "Initialize the model with a constant value: The first step is to initialize the model with a constant value, usually the mean of the target variable. This constant value represents the initial prediction for all observations.\n",
    "\n",
    "Compute the residuals: The next step is to compute the residuals by subtracting the predicted values from the actual target values. The residuals represent the difference between the true value and the predicted value.\n",
    "\n",
    "Train a weak learner on the residuals: A weak learner is trained on the residuals to learn the patterns in the data that were not captured by the previous model. The weak learner is trained to minimize the residual error.\n",
    "\n",
    "Update the model: The weak learner's prediction is added to the existing model, and this updated model is used to predict the target variable. The new predictions are computed by adding the weak learner's prediction to the previous predictions.\n",
    "\n",
    "Repeat steps 2-4: The process is repeated until a stopping criterion is met, such as a maximum number of iterations or a minimum improvement in the loss function.\n",
    "\n",
    "Combine the weak learners: Finally, the weak learners are combined to form the final prediction model. The final prediction is computed as the sum of the initial prediction and the predictions of all the weak learners.\n",
    "\n",
    "The key idea behind Gradient Boosting algorithm is to iteratively improve the prediction by training a series of weak learners on the residuals of the previous model. By doing so, the algorithm is able to capture the complex interactions between the variables and improve the accuracy of the prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd32dc1f-8733-47ca-a7f0-8c87f26548b8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7848faeb-aa11-47f1-a80a-29eaa214207f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
